{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.sql.functions import max, count, col, avg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"KMeansApp\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/Users/sabarnikundu/Downloads/drive_stats_2019_Q1/*.csv\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .load(data_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|smart_1_normalized|\n",
      "+------------------+\n",
      "|             117.0|\n",
      "|              80.0|\n",
      "|              84.0|\n",
      "|              79.0|\n",
      "|             100.0|\n",
      "|              83.0|\n",
      "|              83.0|\n",
      "|              76.0|\n",
      "|              81.0|\n",
      "|              84.0|\n",
      "|             115.0|\n",
      "|              81.0|\n",
      "|              65.0|\n",
      "|              84.0|\n",
      "|              84.0|\n",
      "|             100.0|\n",
      "|             100.0|\n",
      "|             114.0|\n",
      "|             100.0|\n",
      "|             100.0|\n",
      "+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f_data = dataset.select(\"smart_1_normalized\")\n",
    "f_data = f_data.withColumn(\"smart_1_normalized\", f_data[\"smart_1_normalized\"].cast(\"float\"))\n",
    "f_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------+\n",
      "|smart_1_normalized|features|\n",
      "+------------------+--------+\n",
      "|             117.0| [117.0]|\n",
      "|              80.0|  [80.0]|\n",
      "|              84.0|  [84.0]|\n",
      "|              79.0|  [79.0]|\n",
      "|             100.0| [100.0]|\n",
      "|              83.0|  [83.0]|\n",
      "|              83.0|  [83.0]|\n",
      "|              76.0|  [76.0]|\n",
      "|              81.0|  [81.0]|\n",
      "|              84.0|  [84.0]|\n",
      "|             115.0| [115.0]|\n",
      "|              81.0|  [81.0]|\n",
      "|              65.0|  [65.0]|\n",
      "|              84.0|  [84.0]|\n",
      "|              84.0|  [84.0]|\n",
      "|             100.0| [100.0]|\n",
      "|             100.0| [100.0]|\n",
      "|             114.0| [114.0]|\n",
      "|             100.0| [100.0]|\n",
      "|             100.0| [100.0]|\n",
      "+------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Assembler = VectorAssembler(inputCols=[\"smart_1_normalized\"], outputCol=\"features\")\n",
    "f_data = Assembler.setHandleInvalid(\"skip\").transform(f_data).na.drop()\n",
    "f_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of K =  3\n",
      "Within Set Sum of Squared Errors = 55604477.41484555\n",
      "Cluster Centers: \n",
      "[108.68525421]\n",
      "[81.77618785]\n",
      "[74.64574811]\n",
      "Value of K =  4\n",
      "Within Set Sum of Squared Errors = 30803672.288310237\n",
      "Cluster Centers: \n",
      "[74.64574811]\n",
      "[118.76206306]\n",
      "[81.77417717]\n",
      "[100.50193946]\n",
      "Value of K =  5\n",
      "Within Set Sum of Squared Errors = 2955709.6775352894\n",
      "Cluster Centers: \n",
      "[81.48975836]\n",
      "[199.99681451]\n",
      "[116.0926346]\n",
      "[100.37475395]\n",
      "[73.74329291]\n"
     ]
    }
   ],
   "source": [
    "mod = []\n",
    "werror = []\n",
    "kval = [3, 4, 5]\n",
    "for i in kval:\n",
    "    \n",
    "    print('Value of K = ', i)\n",
    "    \n",
    "    kmeans = KMeans().setK(i).setSeed(1)\n",
    "    model = kmeans.fit(f_data)\n",
    "    mod.append(model)\n",
    "\n",
    "    # Evaluate clustering by computing Within Set Sum of Squared Errors.\n",
    "    wssse = model.computeCost(f_data)\n",
    "    werror.append(wssse)\n",
    "    print(\"Within Set Sum of Squared Errors = \" + str(wssse))\n",
    "\n",
    "    # Printing centers of clusters\n",
    "    centers = model.clusterCenters()\n",
    "    print(\"Cluster Centers: \")\n",
    "    for c in centers:\n",
    "        print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mod[1] #choosing k=4 as it gives the reliable distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------+\n",
      "|smart_1_normalized|features|\n",
      "+------------------+--------+\n",
      "|             117.0| [117.0]|\n",
      "|              80.0|  [80.0]|\n",
      "|              84.0|  [84.0]|\n",
      "|              79.0|  [79.0]|\n",
      "|             100.0| [100.0]|\n",
      "|              83.0|  [83.0]|\n",
      "|              83.0|  [83.0]|\n",
      "|              76.0|  [76.0]|\n",
      "|              81.0|  [81.0]|\n",
      "|              84.0|  [84.0]|\n",
      "+------------------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f_data.show(10)\n",
    "\n",
    "predictions = model.transform(f_data)\n",
    "colnames = predictions.schema.names\n",
    "\n",
    "centers = model.clusterCenters()\n",
    "predictions = predictions.rdd.map(lambda x: (x[0], x[2], int(centers[x[2]][0]))).toDF([colnames[0], 'cluster', 'center'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------+------+\n",
      "|smart_1_normalized|cluster|center|\n",
      "+------------------+-------+------+\n",
      "|             117.0|      1|   118|\n",
      "|              80.0|      2|    81|\n",
      "|              84.0|      2|    81|\n",
      "|              79.0|      2|    81|\n",
      "|             100.0|      3|   100|\n",
      "|              83.0|      2|    81|\n",
      "|              83.0|      2|    81|\n",
      "|              76.0|      0|    74|\n",
      "|              81.0|      2|    81|\n",
      "|              84.0|      2|    81|\n",
      "|             115.0|      1|   118|\n",
      "|              81.0|      2|    81|\n",
      "|              65.0|      0|    74|\n",
      "|              84.0|      2|    81|\n",
      "|              84.0|      2|    81|\n",
      "|             100.0|      3|   100|\n",
      "|             100.0|      3|   100|\n",
      "|             114.0|      1|   118|\n",
      "|             100.0|      3|   100|\n",
      "|             100.0|      3|   100|\n",
      "+------------------+-------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------+------+--------+\n",
      "|smart_1_normalized|cluster|center|distance|\n",
      "+------------------+-------+------+--------+\n",
      "|             117.0|      1|   118|     1.0|\n",
      "|              80.0|      2|    81|     1.0|\n",
      "|              84.0|      2|    81|     3.0|\n",
      "|              79.0|      2|    81|     2.0|\n",
      "|             100.0|      3|   100|     0.0|\n",
      "+------------------+-------+------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "colnames = predictions.schema.names\n",
    "dist = predictions.rdd.map(lambda x: (x[0], x[1], x[2], abs(x[2]-x[0]))).toDF(colnames + ['distance'])\n",
    "dist.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|cluster|  average_distance|\n",
      "+-------+------------------+\n",
      "|      0|2.8143438507478464|\n",
      "|      1| 4.696678068723983|\n",
      "|      3|0.5074446171081638|\n",
      "|      2| 1.450772255669031|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Max_dist = dist.groupBy(dist.cluster).agg(avg(dist.distance).alias('average_distance'))\n",
    "Max_dist.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|anomalies|\n",
      "+---------+\n",
      "|     78.0|\n",
      "|     78.0|\n",
      "|     78.0|\n",
      "|     78.0|\n",
      "|     78.0|\n",
      "|     78.0|\n",
      "|     78.0|\n",
      "|     78.0|\n",
      "|     78.0|\n",
      "|     78.0|\n",
      "|     78.0|\n",
      "|     78.0|\n",
      "|     78.0|\n",
      "|     78.0|\n",
      "|     78.0|\n",
      "|     78.0|\n",
      "|     78.0|\n",
      "|     78.0|\n",
      "|     78.0|\n",
      "|     78.0|\n",
      "+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dist.registerTempTable('distances')\n",
    "Max_dist.registerTempTable('clusters')\n",
    "anomalies = spark.sql(\"select smart_1_normalized as anomalies from distances inner join clusters on distances.cluster = clusters.cluster where distances.smart_1_normalized > (distances.center + clusters.average_distance + 1)\")\n",
    "anomalies.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
